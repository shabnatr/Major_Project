{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape =  (120000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group                                              title  \\\n",
       "0      3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4      3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                             article  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/shabi/Desktop/Project/news_article/Newspaper-Article-Classification-master/data/train.csv')\n",
    "df\n",
    "print(\"df.shape = \", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Categories :  4\n",
      "group\n",
      "1    30000\n",
      "2    30000\n",
      "3    30000\n",
      "4    30000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/shabi/Desktop/Project/news_article/Newspaper-Article-Classification-master/data/train.csv')\n",
    "categories = df.groupby('group')\n",
    "print(\"Total Categories : \", categories.ngroups)\n",
    "print(categories.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "df.groupby('group').count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TrpYNXTgKv1"
   },
   "source": [
    "Importing the 20NewsGroups dataset consisting of 11314 articles in the training dataset and 7532  articles in the test dataset accross 20 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0OgD2WhhlN5v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qHqTygNfiJX5"
   },
   "source": [
    "Next we Vectorize the articles in the Corpus.\n",
    "For this we use sci-kit learn's CountVectorizer to create a sparse matrix of the count of each word in an article\n",
    "For better results we then calculate the inverse term frequency for the words using sci-kit learn's TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "E9Nj5GYYQwos",
    "outputId": "4d7e3e12-a3d5-453b-8183-2aadb2f8b0b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "iwG-KEgXlPMB",
    "outputId": "c1920625-74d7-4884-acf8-b65fffe8a372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to vectorize the training data: 2.516 secs\n",
      "Sample sparse matrix after vectorization:\n",
      "  (0, 18299)\t0.138749083899155\n",
      "  (0, 16574)\t0.14155752531572685\n",
      "  (0, 4605)\t0.06332603952480323\n",
      "  (1, 7797)\t0.13724375024886207\n",
      "  (1, 2927)\t0.05212944077716301\n",
      "  (2, 15032)\t0.07834044496813064\n",
      "  (2, 12197)\t0.05168179280403426\n",
      "  (2, 6449)\t0.06812813848609162\n",
      "  (2, 6028)\t0.10554465088856507\n",
      "  (2, 5811)\t0.2878251559842457\n",
      "  (2, 5023)\t0.13698619641739626\n",
      "  (2, 3412)\t0.06228731252083091\n",
      "  (3, 18618)\t0.14195950717692904\n",
      "  (3, 4155)\t0.05353413616615428\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "t1=time.time()\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print (\"Time take to vectorize the training data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Sample sparse matrix after vectorization:\")\n",
    "print (X_train_tfidf[0:4,0:20000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NAOjSZD9iSHC"
   },
   "source": [
    "Having got the sparse matrix, we would now apply classification Algorithms on this vectorized word matrix to predic classes for data in test dataset.\n",
    "Starting with K-Nearest Neighbours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "mSh6vcpgjJMb",
    "outputId": "4dfb7a5b-0b9d-4b17-930a-811f25b28d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 2.258 secs\n",
      "Time take to Predict classes for testing data: 8.412 secs\n",
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6591874668082847"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import neighbors\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', neighbors.KNeighborsClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy:\")\n",
    "np.mean(predicted == test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaZLIzBLl_Zp"
   },
   "source": [
    "Now we apply Support Vector Machine algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "8SeVsWJQlhOm",
    "outputId": "7355bbd3-146c-4fb0-8361-d13f6e71288f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 3.107 secs\n",
      "Time take to Predict classes for testing data: 1.339 secs\n",
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8534253850238981"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy:\")\n",
    "np.mean(predicted == test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDhk52VRl0pD"
   },
   "source": [
    "Now we apply Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "o3hXM3H9kGIe",
    "outputId": "ca32071a-0044-4bb8-bb0e-187cd65522e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 2.391 secs\n",
      "Time take to Predict classes for testing data: 1.342 secs\n",
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy:\")\n",
    "np.mean(predicted == test.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yqLnUNgbmBps"
   },
   "source": [
    "All this while we used Bag-Of-Words technique to vectorize the dataset.\n",
    "Here we apply ngrams technique to create the sparse matrix. Let's have an example as how n-grams is differnt from Bag-of-Words and what it actually does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "ZXp_i8uzoRu1",
    "outputId": "03602872-d63d-48ef-d039-338a02bf074c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words\n",
      "['anagh', 'anmol', 'intelligent', 'is', 'smart']\n",
      "[[2 0 1 1 0]\n",
      " [0 1 0 1 1]]\n",
      "Bi-grams\n",
      "[' a', 'ag', 'an', 'gh', 'h ', 'l ', 'mo', 'na', 'nm', 'ol']\n",
      "[[1 1 1 1 1 0 0 1 0 0]\n",
      " [1 0 1 0 0 1 1 0 1 1]]\n",
      "Tri-grams\n",
      "[' an', 'agh', 'ana', 'anm', 'gh ', 'mol', 'nag', 'nmo', 'ol ']\n",
      "[[1 1 1 0 1 0 1 0 0]\n",
      " [1 0 0 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer()\n",
    "counts = ngram_vectorizer.fit_transform(['Anagh Anagh is intelligent', 'Anmol is smart'])\n",
    "print(\"Bag-of-Words\")\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
    "counts = ngram_vectorizer.fit_transform(['Anagh', 'Anmol'])\n",
    "print(\"Bi-grams\")\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))\n",
    "counts = ngram_vectorizer.fit_transform(['Anagh', 'Anmol'])\n",
    "print(\"Tri-grams\")\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHgbmYdlrimI"
   },
   "source": [
    "What if we need to apply Bag-of-2grams, or in other words club two consecutive words in a document, then vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "lraA3HVTrvp9",
    "outputId": "3ea993f3-e2d5-43eb-c50c-888185d088fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-2grams\n",
      "['be prayagraj', 'is allahabad', 'it is', 'it will', 'today it', 'tomorrow it', 'will be']\n",
      "[[0 1 1 0 1 0 0]\n",
      " [1 0 0 1 0 1 1]]\n",
      "Time take to vectorize the training data: 10.736 secs\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "counts = ngram_vectorizer.fit_transform(['Today it is Allahabad', 'Tomorrow it will be Prayagraj'])\n",
    "print(\"Bag-of-2grams\")\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))\n",
    "t1=time.time()\n",
    "count_vect = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print (\"Time take to vectorize the training data:\", round(time.time()-t1, 3), \"secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2GhI2nohoSSf"
   },
   "source": [
    "Now we will apply n-grams on our dataset and will then apply SVM classification algorithm. We will compare the accuracies for uni-gram, bi-gram and tri-gram vectorization on character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "ONMMNbOumXRY",
    "outputId": "53281e0c-d7a7-4771-ba24-ec08b73658ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR CHARACTER LEVEL n-GRAMS\n",
      "-------------------\n",
      "UNI-GRAMS\n",
      "Time take to Vectorize training data and train model: 7.145 secs\n",
      "Accuracy :  0.1421933085501859\n",
      "--------------------\n",
      "BI-GRAMS\n",
      "Time take to Vectorize training data and train model: 9.701 secs\n",
      "Accuracy :  0.6618428040361126\n",
      "--------------------\n",
      "TRI-GRAMS\n",
      "Time take to Vectorize training data and train model: 11.936 secs\n",
      "Accuracy :  0.8151885289431758\n",
      "--------------------\n",
      "4-GRAMS\n",
      "Time take to Vectorize training data and train model: 14.962 secs\n",
      "Accuracy :  0.8429368029739777\n",
      "--------------------\n",
      "5-GRAMS\n",
      "Time take to Vectorize training data and train model: 16.344 secs\n",
      "Time take to Vectorize training data and train model: 5.424 secs\n",
      "Accuracy :  0.8422729686670207\n",
      "--------------------\n",
      "6-GRAMS\n",
      "Time take to Vectorize training data and train model: 16.959 secs\n",
      "Accuracy :  0.8381571959638874\n"
     ]
    }
   ],
   "source": [
    "print(\"FOR CHARACTER LEVEL n-GRAMS\")\n",
    "print(\"-------------------\")\n",
    "print(\"UNI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(1, 1))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"BI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "\n",
    "print(\"--------------------\")\n",
    "print(\"TRI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"4-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(4, 4))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"5-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"6-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(6, 6))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3P5BQlbsMI3"
   },
   "source": [
    "Now we apply N-Grams with Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "0P7GK7f5-BqZ",
    "outputId": "9d5a14c3-7788-4673-aea1-dc81a2efff8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 11.375 secs\n",
      "Time take to Predict classes for Test data: 5.401 secs\n",
      "Accuracy :  0.7015400955921403\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import neighbors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Predict classes for Test data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cHe5EOlnsSOt"
   },
   "source": [
    "Now we apply N-Grams with KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "yNFPEfbuBT13",
    "outputId": "692a47d4-cfc1-4999-9f72-8742e1bd23d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 18.763 secs\n",
      "Time take to predict classes for Test data: 37.134 secs\n",
      "Accuracy :  0.5728890069038768\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', neighbors.KNeighborsClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to predict classes for Test data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PADWu_yqtZr"
   },
   "source": [
    "Now applying Bag-of-1gram(Same as bag of words), Bag-of-2grams and Bag-of-3grams to our dataset this time grouping words together instead of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "svJR_6Anqs2m",
    "outputId": "829a60ba-7f43-4947-e2cf-36a20c9251a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD LEVEL n-GRAMS\n",
      "--------------------\n",
      "UNI-GRAMS\n",
      "Time take to Vectorize training data and train model: 3.274 secs\n",
      "Accuracy :  0.8524960169941582\n",
      "--------------------\n",
      "BI-GRAMS\n",
      "Time take to Vectorize training data and train model: 10.697 secs\n",
      "Accuracy :  0.8028412108337759\n",
      "--------------------\n",
      "TRI-GRAMS\n",
      "Time take to Vectorize training data and train model: 16.093 secs\n",
      "Accuracy :  0.716542750929368\n"
     ]
    }
   ],
   "source": [
    "print(\"WORD LEVEL n-GRAMS\")\n",
    "print(\"--------------------\")\n",
    "print(\"UNI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(1, 1))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"BI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(2, 2))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"TRI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(3, 3))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TUjUnpaKvXzk"
   },
   "source": [
    "Now we apply Word level CNN on the same dataset and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "colab_type": "code",
    "id": "ZOe9kwl0Yjcr",
    "outputId": "2b93741f-173e-4df0-cb46-04de9931930b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               25600512  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 25,873,428\n",
      "Trainable params: 25,873,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/10\n",
      "10182/10182 [==============================] - 29s 3ms/step - loss: 0.9909 - accuracy: 0.7565 - val_loss: 0.3868 - val_accuracy: 0.9231\n",
      "Epoch 2/10\n",
      "10182/10182 [==============================] - 28s 3ms/step - loss: 0.2001 - accuracy: 0.9829 - val_loss: 0.3534 - val_accuracy: 0.9099\n",
      "Epoch 3/10\n",
      "10182/10182 [==============================] - 28s 3ms/step - loss: 0.0882 - accuracy: 0.9930 - val_loss: 0.4738 - val_accuracy: 0.9081\n",
      "Epoch 4/10\n",
      "10182/10182 [==============================] - 28s 3ms/step - loss: 0.1415 - accuracy: 0.9922 - val_loss: 0.3726 - val_accuracy: 0.9240\n",
      "Epoch 5/10\n",
      "10182/10182 [==============================] - 28s 3ms/step - loss: 0.0362 - accuracy: 0.9954 - val_loss: 0.3385 - val_accuracy: 0.9178\n",
      "Epoch 6/10\n",
      "10182/10182 [==============================] - 28s 3ms/step - loss: 0.0826 - accuracy: 0.9949 - val_loss: 0.4752 - val_accuracy: 0.9019\n",
      "Epoch 7/10\n",
      "10182/10182 [==============================] - 28s 3ms/step - loss: 0.0448 - accuracy: 0.9966 - val_loss: 0.3811 - val_accuracy: 0.9231\n",
      "Epoch 8/10\n",
      "10182/10182 [==============================] - 28s 3ms/step - loss: 0.0592 - accuracy: 0.9964 - val_loss: 0.4110 - val_accuracy: 0.9223\n",
      "Epoch 9/10\n",
      "10182/10182 [==============================] - 28s 3ms/step - loss: 0.0327 - accuracy: 0.9975 - val_loss: 0.3637 - val_accuracy: 0.9196\n",
      "Epoch 10/10\n",
      "10182/10182 [==============================] - 28s 3ms/step - loss: 0.0218 - accuracy: 0.9978 - val_loss: 0.3852 - val_accuracy: 0.9187\n",
      "7532/7532 [==============================] - 4s 490us/step\n",
      "Time take to Predict classes for testing data: 3.695 secs\n",
      "Test accuracy: 0.8197026252746582\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "np.random.seed(1237)\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "label_index = train.target\n",
    "label_names = train.target_names\n",
    "labelled_files = train.filenames\n",
    " \n",
    "data_tags = [\"filename\",\"category\",\"news\"]\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],train.data[i]))\n",
    "    i += 1\n",
    "    \n",
    "train_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "\n",
    "label_index = test.target\n",
    "label_names = test.target_names\n",
    "labelled_files = test.filenames\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],test.data[i]))\n",
    "    i += 1\n",
    "    \n",
    "test_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "\n",
    "train_posts = train_data['news'][:]\n",
    "train_tags = train_data['category'][:]\n",
    "train_files_names = train_data['filename'][:]\n",
    " \n",
    "test_posts = test_data['news'][:]\n",
    "test_tags = test_data['category'][:]\n",
    "test_files_names = test_data['filename'][:]\n",
    "\n",
    "# 20 news groups\n",
    "num_labels = 20\n",
    "vocab_size = 50000\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    " \n",
    "\n",
    "\t\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.333))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "t1=time.time()\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyQhdnforgk8"
   },
   "source": [
    "Now we apply CNN on data pre processed with N-Grams, taking value of N as 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1198
    },
    "colab_type": "code",
    "id": "4dbwgJwKGkNH",
    "outputId": "e5120489-5006-427f-f89a-996b40852f2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               15360512  \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 15,633,428\n",
      "Trainable params: 15,633,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/20\n",
      "10182/10182 [==============================] - 63s 6ms/step - loss: 1.8934 - accuracy: 0.5464 - val_loss: 0.6793 - val_accuracy: 0.8366\n",
      "Epoch 2/20\n",
      "10182/10182 [==============================] - 17s 2ms/step - loss: 0.2477 - accuracy: 0.9416 - val_loss: 0.4943 - val_accuracy: 0.8516\n",
      "Epoch 3/20\n",
      "10182/10182 [==============================] - 17s 2ms/step - loss: 0.0374 - accuracy: 0.9941 - val_loss: 0.4822 - val_accuracy: 0.8666\n",
      "Epoch 4/20\n",
      "10182/10182 [==============================] - 18s 2ms/step - loss: 0.0151 - accuracy: 0.9977 - val_loss: 0.5043 - val_accuracy: 0.8719\n",
      "Epoch 5/20\n",
      "10182/10182 [==============================] - 19s 2ms/step - loss: 0.0095 - accuracy: 0.9981 - val_loss: 0.5013 - val_accuracy: 0.8684\n",
      "Epoch 6/20\n",
      "10182/10182 [==============================] - 19s 2ms/step - loss: 0.0088 - accuracy: 0.9984 - val_loss: 0.5186 - val_accuracy: 0.8675\n",
      "Epoch 7/20\n",
      "10182/10182 [==============================] - 18s 2ms/step - loss: 0.0067 - accuracy: 0.9988 - val_loss: 0.5391 - val_accuracy: 0.8648\n",
      "Epoch 8/20\n",
      "10182/10182 [==============================] - 18s 2ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 0.5157 - val_accuracy: 0.8693\n",
      "Epoch 9/20\n",
      "10182/10182 [==============================] - 18s 2ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.5349 - val_accuracy: 0.8684\n",
      "Epoch 10/20\n",
      "10182/10182 [==============================] - 17s 2ms/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.5429 - val_accuracy: 0.8640\n",
      "Epoch 11/20\n",
      "10182/10182 [==============================] - 18s 2ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.5495 - val_accuracy: 0.8693\n",
      "Epoch 12/20\n",
      "10182/10182 [==============================] - 18s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.5612 - val_accuracy: 0.8622\n",
      "Epoch 13/20\n",
      "10182/10182 [==============================] - 18s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.5485 - val_accuracy: 0.8675\n",
      "Epoch 14/20\n",
      "10182/10182 [==============================] - 19s 2ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.5613 - val_accuracy: 0.8684\n",
      "Epoch 15/20\n",
      "10182/10182 [==============================] - 18s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.5611 - val_accuracy: 0.8684\n",
      "Epoch 16/20\n",
      "10182/10182 [==============================] - 18s 2ms/step - loss: 0.0029 - accuracy: 0.9988 - val_loss: 0.5835 - val_accuracy: 0.8622\n",
      "Epoch 17/20\n",
      "10182/10182 [==============================] - 18s 2ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.5804 - val_accuracy: 0.8675\n",
      "Epoch 18/20\n",
      "10182/10182 [==============================] - 18s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.5683 - val_accuracy: 0.8746\n",
      "Epoch 19/20\n",
      "10182/10182 [==============================] - 18s 2ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.5781 - val_accuracy: 0.8684\n",
      "Epoch 20/20\n",
      "10182/10182 [==============================] - 21s 2ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.5844 - val_accuracy: 0.8666\n",
      "7532/7532 [==============================] - 3s 346us/step\n",
      "Time take to Predict classes for testing data: 3.264 secs\n",
      "Test accuracy: 0.06186935678124428\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "np.random.seed(1237)\n",
    "import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(2, 2), max_features=30000)\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(2, 2), max_features=30000)\n",
    "X_train_counts = count_vect.fit_transform(test.data)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "label_index = train.target\n",
    "label_names = train.target_names\n",
    "labelled_files = train.filenames\n",
    " \n",
    "data_tags = [\"filename\",\"category\",\"news\"]\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],train.data[i]))\n",
    "    i += 1\n",
    "    \n",
    "train_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "\n",
    "label_index = test.target\n",
    "label_names = test.target_names\n",
    "labelled_files = test.filenames\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],test.data[i]))\n",
    "    i += 1\n",
    "    \n",
    "test_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "\n",
    "train_posts = train_data['news'][:]\n",
    "train_tags = train_data['category'][:]\n",
    "train_files_names = train_data['filename'][:]\n",
    " \n",
    "test_posts = test_data['news'][:]\n",
    "test_tags = test_data['category'][:]\n",
    "test_files_names = test_data['filename'][:]\n",
    "\n",
    "# 20 news groups\n",
    "num_labels = 20\n",
    "vocab_size = 50000\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    " \n",
    "\n",
    "\t\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(30000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.333))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(X_train_tfidf.todense(), y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=20,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "t1=time.time()\n",
    "score = model.evaluate(X_test_tfidf, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Text_Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
